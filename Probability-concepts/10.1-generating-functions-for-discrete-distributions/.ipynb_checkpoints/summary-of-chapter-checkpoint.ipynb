{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We'll see in this section that $\\mu$ and $\\sigma$ aren't enough to describe a density\n",
    "\n",
    "## Example\n",
    "\n",
    "### Suppose $X$ and $Y$ are random variables, with distributions:\n",
    "\n",
    "# $p_{X} = \\bigl(\\begin{smallmatrix}1 & 2 & 3 & 4 & 5 & 6\\\\ 0 & 1/4 & 1/2 & 0 & 0 & 1/4\\end{smallmatrix}\\bigr)$\n",
    "\n",
    "# $p_{Y} = \\bigl(\\begin{smallmatrix}1 & 2 & 3 & 4 & 5 & 6\\\\ 1/4 & 0 & 0 & 1/2 & 1/4 & 0\\end{smallmatrix}\\bigr)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $E(X) = (2)(1/4)+ (3)(1/2) + (6)(1/4) = 7/2$\n",
    "\n",
    "### $E(X^{2}) = (2^{2})(1/4)+ (3^{2})(1/2) + (6^{2})(1/4) = 29/2 \\implies V(X) = 29/2 - 49/4 = 9/4$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $E(Y) = (1)(1/4) + (4)(1/2) + (5)(1/4) = 7/2$\n",
    "\n",
    "### $E(Y^{2}) = (1^{2})(1/4) + (4^{2})(1/2) + (5^{2})(1/4) = 29/2 \\implies V(Y) = 9/4$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So, we can see that the mean and variance are the same, but the distributions are very different"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our new question is: what else do we need to know about $X$ and $Y$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "# *Moments*\n",
    "\n",
    "# $\\mu_{k} = k^{th}\\text{ moment of the mean} = E(X^{k}) = \\sum(x_{i})^{k}p_{X}(x_{i})$\n",
    "\n",
    "# $\\implies \\mu = \\mu_{1}$ and $\\sigma^{2} = \\mu_{2} - \\mu_{1}^{2}$\n",
    "\n",
    "### So if the know the first two moments, we know the mean and variance\n",
    "\n",
    "## But if we know *all* the moments of the mean, we know everything about the density"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____\n",
    "\n",
    "# *Moment Generating Functions*\n",
    "\n",
    "### Let's define a function $g(t)$ as:\n",
    "\n",
    "# $g(t) = E(e^{tX}) = \\sum \\frac{\\mu_{k}t^{k}}{k!} = E \\left ( \\sum \\frac{X^{k}t^{k}}{k!} \\right ) = \\sum e^{tx_{i}}p(x_{i})$\n",
    "\n",
    "### This function $g(t)$ is the *moment generating function for $X$*\n",
    "\n",
    "# If we differentiate $g(t)$ $k$-times and set $t=0$, then we get $\\mu_{k}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______\n",
    "\n",
    "# *Examples of Moment Generating Functions*\n",
    "\n",
    "## A)\n",
    "\n",
    "### Suppose $X$ has range $[1,2,...,n]$ and $p_{X}(x) = \\frac{1}{n}$\n",
    "\n",
    "### Then $g(t) = \\sum e^{kt}\\frac{1}{n} = \\frac{e^{t} + e^{2t} + ... + e^{nt}}{n} = \\frac{e^{t}(e^{nt}-1)}{n(e^{t} - 1)}$\n",
    "\n",
    "### $\\implies g^{'}(t) = \\frac{1e^{t} + 2e^{2t}+...+ne^{nt}}{n} \\implies g^{'}(0) = \\frac{1+2+...+n}{n} = \\frac{n+1}{2} = \\mu_{1}$\n",
    "\n",
    "### $\\implies g^{''}(t) = \\frac{1^{2}e^{t} + 2^{2}e^{2t}+...+n^{2}e^{nt}}{n} \\implies g^{''}(0) = \\frac{1^{2}+2^{2}+...+n^{2}}{n} = \\frac{(n+1)(2n+1)}{6} = \\mu_{2}$\n",
    "\n",
    "### $\\implies \\sigma^{2} =  \\mu_{2} - ( \\mu_{1})^{2} = \\frac{(n+1)(2n+1)}{6} - \\left ( \\frac{n+1}{2}\\right )^{2} = \\frac{n^{2}-1}{12}$\n",
    "\n",
    "\n",
    "\n",
    "## B)\n",
    "\n",
    "### Suppose $X$ has range $[1,2,...,n]$ and $p_{X}(k) = \\binom{n}{k}p^{k}(1-p)^{n-k}$\n",
    "\n",
    "### Then $g(t) = \\sum e^{kt}\\binom{n}{k}p^{k}(1-p)^{n-k} = \\sum \\binom{n}{k}(e^{t}p)^{k}(1-p)^{n-k} = (pe^{t} + q)^{n}$\n",
    "\n",
    "### $\\implies g^{'}(t) = n(pe^{t} + q)^{n-1}pe^{t} \\implies g^{'}(0) = np$\n",
    "\n",
    "### $\\implies g^{''}(t) = n(n-1)(pe^{t} + q)^{n-2}pe^{t} \\implies g^{''}(0) = np^{2}(n-1) + np$\n",
    "\n",
    "### $\\implies \\sigma^{2} = \\mu_{2} - \\mu_{1}^{2} = np^{2}(n-1) + np - (np)^{2} = np(1-p)$ as expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____\n",
    "\n",
    "# Theorem 10.1\n",
    "\n",
    "### Let $X$ be a discrete random variable with finite range, density $p_{X}$, and moment generating function $g$\n",
    "\n",
    "### Then $g$ is uniquely determined by $p_{X}$, and vice-versa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______\n",
    "\n",
    "# *Ordinary Generating Functions*\n",
    "\n",
    "### In a lot of cases, $X$ will be over the non-negative integers $[0,1,2,...,n]$\n",
    "\n",
    "### So if we let $z = e^{t}$ then $\\sum e^{kt}p(k) = \\sum z^{k}p(k)$\n",
    "\n",
    "## We define $h(z) = \\sum z^{k}p(k)$\n",
    "\n",
    "## $h(z)$ is called the *ordinary generating function of $X$*\n",
    "\n",
    "## Remarks\n",
    "\n",
    "### 1. $h(1) = g(0)$\n",
    "\n",
    "### 2. $h^{'}(1) = g^{'}(0)$\n",
    "\n",
    "### 3. $h^{''}(1) = g^{''}(0) - g^{'}(0)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______\n",
    "\n",
    "# *Properties*\n",
    "\n",
    "## $Y = X+a \\implies g_{Y}(t) = e^{ta}\\cdot g_{X}(t)$\n",
    "\n",
    "## $Y = bX \\implies g_{Y}(t) = g_{X}(bt)$\n",
    "\n",
    "## $Z=X+Y \\implies g_{Z}(t) = g_{X}(t)g_{Y}(t); h_{Z}(z) = h_{X}(z)h_{Y}(z)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "_____\n",
    "\n",
    "# *Heads or Tails Example*\n",
    "\n",
    "### Peter and James are gambling on coin flips\n",
    "\n",
    "### For every heads, Peter gets a dollar. For every tails, he loses a dollar\n",
    "\n",
    "### We want to know: \"When is Peter first in the lead?\"\n",
    "\n",
    "### Let $X_{k}$ describe the payoff of the $k$th flip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $ \\implies p_{X_{k}} = \\left\\{\\begin{matrix}1/2 & x = -1\\\\ 1/2 & x = 1\\end{matrix}\\right.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then, $S_{n}$ represents Peter's fortune after $n$ flips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This means that if time $k$ is the first time that Peter is winning, then $S_{j}\\leq0$ for all $j<k$ and $S_{k}=1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### If the first time Peter is winning is at time $k=1$, then $S_{1} = 1$\n",
    "\n",
    "### Otherwise, if $k>1$, $S_{1} = -1$ and $S_{k-1} = 0$\n",
    "\n",
    "### Furthermore, $S_{i}$ could also equal zero, but $S_{i+1}$ was equal to -1 (since Peter lost the following flip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let $m$ be the smallest $i$ value such that $S_{m} = 0 \\implies S_{j}<0$ for all $j<m$ (otherwise, there would be some $j$ smaller than $m$ such that $S_{j}=0$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let $p = 0.5$ and $q=1-p = 0.5$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let $Y$ be the random variable representing the first time Peter is in the lead\n",
    "\n",
    "### It's impossible for Peter to gain the lead after an even number of flips $\\implies P(Y=n) = 0$ if $n$ is even"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $P(Y=1) = 0.5$\n",
    "\n",
    "### Then, for all remaining possible values of $n$\n",
    "\n",
    "### $P(Y=n) = q...$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tired of this shit, will finish up later"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
